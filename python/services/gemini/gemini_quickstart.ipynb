{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the AI Gemini API\n",
    "\n",
    "## Prerequisites\n",
    "- Use Python 3.9\n",
    "- Install the google-generativeai SDK\n",
    "- Get an API key from Google AI Studio https://aistudio.google.com/app/apikey\n",
    "- Copy your key and move it to the home directory of the VM or device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=9, micro=5, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "# verify python 3.9\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "assert sys.version_info >= (3, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrade to Python 3.9 Ubunto\n",
    "\n",
    "- Search if the version of python is available in the current Ubuntu repos\n",
    "\n",
    "```bash\n",
    "apt search python3.9\n",
    "```\n",
    "\n",
    "- if packages are listed. Install with the following command\n",
    "\n",
    "```bash\n",
    "sudo apt install python3.9\n",
    "```\n",
    "\n",
    "- If the package is not listed, download the entire file and install from the command line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your API key from Google AI Studio\n",
    "\n",
    "[https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "- Copy the key\n",
    "\n",
    "## Update the API key in your environment\n",
    "\n",
    "Using a terminal, run the save_key.sh bash file and enter the key at the prompt.\n",
    "\n",
    "This script will do the following:\n",
    "\n",
    "- Create a $Home/.gcp folder\n",
    "- Save the API key in the gemini.key file by running this bash file\n",
    "- Add a new environment variable GEMINI_KEY\n",
    "\n",
    "```bash\n",
    "./save_key.sh\n",
    "```\n",
    "\n",
    "> For production environments, set the correct permissions for that file or use a keyvault (recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a virtual environment\n",
    "\n",
    "- We recommend the use of a virtual environment to run this.\n",
    "```bash\n",
    "pip install pipenv\n",
    "pipenv shell\n",
    "```\n",
    "\n",
    "> You can run this without `pipenv` by installing only the dependencies locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Gemini API dependencies from the terminal\n",
    "\n",
    "- Use PIPEnv to update the depencies\n",
    "```bash\n",
    "pipenv shell\n",
    "pipenv sync\n",
    "```\n",
    "> pipenv sync installs the dependencies from the Pipfile\n",
    "\n",
    "\n",
    "- Or manually install the dependencies (No pipEnv)\n",
    "\n",
    "## Install the API requests module\n",
    "```bash\n",
    "pipenv install -q google-generativeai\n",
    "pipenv install requests\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# import the GEMINI API\n",
    "import google.generativeai as gemini\n",
    "\n",
    "# get the key reference\n",
    "api_key = os.getenv('GEMINI_KEY')\n",
    "model_name = 'gemini-pro'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LLM Models for Code Generation**\n",
      "\n",
      "**What are LLM Models?**\n",
      "\n",
      "* Large Language Models (LLMs) are deep learning models trained on massive datasets of text and code.\n",
      "* They can understand natural language, generate text, and write code with human-like accuracy.\n",
      "\n",
      "**Benefits of LLMs for Code Generation**\n",
      "\n",
      "* **Increased productivity:** LLM models can automate repetitive coding tasks, freeing developers for higher-level work.\n",
      "* **Improved code quality:** LLM models can generate code that is syntactically correct and follows best practices.\n",
      "* **Reduced development time:** LLM models can speed up development by reducing the time spent writing code from scratch.\n",
      "\n",
      "**How to Use LLM Models for Code Generation**\n",
      "\n",
      "* **Identify suitable use cases:** LLM models are best suited for automating routine tasks, such as generating boilerplate code or documentation.\n",
      "* **Choose an LLM model:** There are several LLM models available, such as GPT-3, Codex, and Gemini. Choose one that fits your requirements and budget.\n",
      "* **Provide clear instructions:** Use natural language commands to describe the code you want to generate. Include details such as the desired function, data types, and constraints.\n",
      "* **Use code editing tools:** Many LLM models come with built-in code editors that provide syntax highlighting and error checking.\n",
      "* **Iterate and refine:** LLM models are not perfect. Be prepared to iterate on your code generation requests and provide feedback to improve the results.\n",
      "\n",
      "**Examples of Code Generation Tasks**\n",
      "\n",
      "* Generating boilerplate code for common tasks\n",
      "* Creating documentation for code functions\n",
      "* Translating code from one language to another\n",
      "* Writing unit tests for code modules\n",
      "* Generating code snippets for specific requirements\n",
      "\n",
      "**Best Practices**\n",
      "\n",
      "* **Start with small tasks:** Begin by using LLM models for simple code generation tasks to gain familiarity and confidence.\n",
      "* **Provide sufficient context:** Provide clear instructions and any relevant documentation to help the LLM model understand your requirements.\n",
      "* **Review and test code:** Always review and test the generated code before deploying it in production.\n",
      "* **Use ethical considerations:** Consider the ethical implications of using LLMs for code generation, such as potential bias or job displacement.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "LLM models offer powerful capabilities for code generation that can significantly enhance developer productivity and improve code quality. By following best practices and identifying suitable use cases, developers can leverage LLM models to streamline their development process and achieve better results.\n"
     ]
    }
   ],
   "source": [
    "if api_key is not None:\n",
    "    gemini.configure(api_key=api_key)\n",
    "\n",
    "    # create a service instance    \n",
    "    model = gemini.GenerativeModel(model_name)\n",
    "    prompt = 'We are in a presentation about LLM models and how to use them to help developers generate code. Can you help us on the subject?'\n",
    "    \n",
    "    # generate content\n",
    "    result = model.generate_content(prompt)\n",
    "    print(result.text)\n",
    "else:\n",
    "    print('The key was not loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def max_number(list1):\n",
      "    max_num = list1[0]\n",
      "    for x in list1:\n",
      "        if x > max_num:\n",
      "            max_num = x\n",
      "    return max_num\n",
      "\n",
      "\n",
      "# Driver code to test above\n",
      "list1 = [10, 20, 4, 5, 6, 7, 8, 9]\n",
      "print(\"Maximum element is:\", max_number(list1))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "bad_prompt = \"Write some code that select the biggest number from a list\"\n",
    "\n",
    "result = model.generate_content(bad_prompt)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def find_max(nums):\n",
      "  \"\"\"\n",
      "  Finds the largest integer in a list of integers.\n",
      "\n",
      "  Parameters:\n",
      "    nums: A list of integers.\n",
      "\n",
      "  Returns:\n",
      "    The largest integer in the list.\n",
      "  \"\"\"\n",
      "\n",
      "  # Initialize the maximum value to the first element in the list.\n",
      "  max_value = nums[0]\n",
      "\n",
      "  # Iterate over the remaining elements in the list.\n",
      "  for num in nums[1:]:\n",
      "    # If the current element is greater than the maximum value, update the maximum value.\n",
      "    if num > max_value:\n",
      "      max_value = num\n",
      "\n",
      "  # Return the maximum value.\n",
      "  return max_value\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "improved_prompt = \"Write a Python function with the name find_max that takes a list of integers as input and returns the largest integer in the list.\"\n",
    "result = model.generate_content(improved_prompt)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exit and remove the virtual environment\n",
    "\n",
    "```bash\n",
    "exit\n",
    "pipenv --rm\n",
    "```\n",
    "> Make sure to exit the pipenv shell and remove the virtual environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
