{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976711e9",
   "metadata": {},
   "source": [
    "Manufacturing AI Agent\n",
    "\n",
    "- Install Dependencies\n",
    "  \n",
    "```bash \n",
    "pipenv shell\n",
    "pipenv install langgraph langchain  \n",
    "pipenv install langchain-google-genai==2.1.5 \n",
    "pipenv google-ai-generativelanguage==0.6.18\n",
    "\n",
    "// check the version\n",
    "python3 -m langchain_core.sys_info\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855ef8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozkary/.local/share/virtualenvs/chat-gpt-2H0-IQSn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage \n",
    "# from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e6d346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System Information\n",
      "------------------\n",
      "> OS:  Linux\n",
      "> OS Version:  #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025\n",
      "> Python Version:  3.9.5 (default, Nov 23 2021, 15:27:38) \n",
      "[GCC 9.3.0]\n",
      "\n",
      "Package Information\n",
      "-------------------\n",
      "> langchain_core: 0.3.66\n",
      "> langchain: 0.3.26\n",
      "> langsmith: 0.4.1\n",
      "> langchain_google_genai: 2.0.10\n",
      "> langchain_text_splitters: 0.3.8\n",
      "> langgraph_sdk: 0.1.70\n",
      "\n",
      "Optional packages not installed\n",
      "-------------------------------\n",
      "> langserve\n",
      "\n",
      "Other Dependencies\n",
      "------------------\n",
      "> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n",
      "> filetype: 1.2.0\n",
      "> google-generativeai: 0.8.5\n",
      "> httpx: 0.28.1\n",
      "> httpx>=0.25.2: Installed. No version info available.\n",
      "> jsonpatch<2.0,>=1.33: Installed. No version info available.\n",
      "> langchain-anthropic;: Installed. No version info available.\n",
      "> langchain-aws;: Installed. No version info available.\n",
      "> langchain-azure-ai;: Installed. No version info available.\n",
      "> langchain-cohere;: Installed. No version info available.\n",
      "> langchain-community;: Installed. No version info available.\n",
      "> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n",
      "> langchain-core<1.0.0,>=0.3.66: Installed. No version info available.\n",
      "> langchain-deepseek;: Installed. No version info available.\n",
      "> langchain-fireworks;: Installed. No version info available.\n",
      "> langchain-google-genai;: Installed. No version info available.\n",
      "> langchain-google-vertexai;: Installed. No version info available.\n",
      "> langchain-groq;: Installed. No version info available.\n",
      "> langchain-huggingface;: Installed. No version info available.\n",
      "> langchain-mistralai;: Installed. No version info available.\n",
      "> langchain-ollama;: Installed. No version info available.\n",
      "> langchain-openai;: Installed. No version info available.\n",
      "> langchain-perplexity;: Installed. No version info available.\n",
      "> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n",
      "> langchain-together;: Installed. No version info available.\n",
      "> langchain-xai;: Installed. No version info available.\n",
      "> langsmith-pyo3: Installed. No version info available.\n",
      "> langsmith>=0.1.17: Installed. No version info available.\n",
      "> langsmith>=0.3.45: Installed. No version info available.\n",
      "> openai-agents: Installed. No version info available.\n",
      "> opentelemetry-api: Installed. No version info available.\n",
      "> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n",
      "> opentelemetry-sdk: Installed. No version info available.\n",
      "> orjson: 3.10.18\n",
      "> orjson>=3.10.1: Installed. No version info available.\n",
      "> packaging: 24.2\n",
      "> packaging<25,>=23.2: Installed. No version info available.\n",
      "> pydantic: 2.11.7\n",
      "> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n",
      "> pydantic>=2.7.4: Installed. No version info available.\n",
      "> pytest: Installed. No version info available.\n",
      "> PyYAML>=5.3: Installed. No version info available.\n",
      "> requests: 2.32.4\n",
      "> requests-toolbelt: 1.0.0\n",
      "> requests<3,>=2: Installed. No version info available.\n",
      "> rich: Installed. No version info available.\n",
      "> SQLAlchemy<3,>=1.4: Installed. No version info available.\n",
      "> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n",
      "> typing-extensions>=4.7: Installed. No version info available.\n",
      "> zstandard: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m langchain_core.sys_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfded378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_KEY\")\n",
    "# print(GEMINI_KEY)\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.3,  # .1-.3 more deterministic\n",
    "    google_api_key=GEMINI_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efea8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- System Prompt ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a specialized control chart monitoring agent in a manufacturing environment.\n",
    "Your job is to assess whether a process is trending out of control based on sensor readings and SPC rules.\n",
    "Ignore the thought element.\n",
    "\n",
    "Use this format:\n",
    "@observe: Acknowledge the new reading.\n",
    "@reason: Explain trend status or SPC rule violations.\n",
    "@act: Recommend an action (e.g., monitor, alert supervisor, prep shutdown).\n",
    "\"\"\"\n",
    "\n",
    "# --- Sample Input ---\n",
    "sensor_data = {\n",
    "    \"sensor\": \"vibration_motor_3A\",\n",
    "    \"timestamp\": \"2025-06-18T11:47:09Z\",\n",
    "    \"value\": 0.89,\n",
    "    \"unit\": \"mm/s\",\n",
    "    \"ucl\": 0.90,\n",
    "    \"lcl\": 0.50,\n",
    "    \"trend_window\": [0.91, 0.94, 0.86, 0.89],\n",
    "    \"alert_enabled\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca83b6",
   "metadata": {},
   "source": [
    "## üîÅ LangGraph Agent Workflow\n",
    "\n",
    "This section explains how LangGraph is used to define and orchestrate an AI agent pipeline. The agent performs a structured SPC (Statistical Process Control) analysis over sensor data using a modular and reactive graph-based system.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Node Definitions\n",
    "\n",
    "Each function below is defined as a **LangGraph node**, which receives and returns state (a dictionary) to carry data through the workflow.\n",
    "\n",
    "- **`ingest(state)`**  \n",
    "  Extracts and standardizes the input under the key `\"sensor_data\"`.\n",
    "\n",
    "- **`analyze(state)`**  \n",
    "  Formats the sensor data and passes it as a prompt to the LLM (`llm.invoke(...)`). It stores a record of input/output in memory, and outputs the LLM‚Äôs structured response (e.g., `@observe`, `@reason`, `@act`).\n",
    "\n",
    "- **`output(state)`**  \n",
    "  Displays the response to the terminal. It serves as the terminal node in the graph.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Graph Construction\n",
    "\n",
    "Using `StateGraph`, the following steps are taken to build and compile the agent:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "  Start([Start])\n",
    "  Ingest([üü¢ ingest<br>Receive and pass sensor data])\n",
    "  Analyze([üß† analyze<br>Format data, invoke LLM, store response])\n",
    "  Output([üì§ output<br>Display agent response])\n",
    "  End([üèÅ END])\n",
    "\n",
    "  Start --> Ingest\n",
    "  Ingest --> Analyze\n",
    "  Analyze --> Output\n",
    "  Output --> End\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03e650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def display_agent_response(response): \n",
    "    print(\"===== ü§ñ AI AGENT RESPONSE =====\\n\")\n",
    "    try:\n",
    "        # Convert string to dictionary\n",
    "        response_dict = json.loads(response)           \n",
    "        print(f\"\\nüì° @observe:\\n{response_dict.get('@observe', 'N/A')}\")\n",
    "        print(f\"\\nüß† @reason:\\n{response_dict.get('@reason', 'N/A')}\")\n",
    "        print(f\"\\n‚öôÔ∏è @act:\\n{response_dict.get('@act', 'N/A')}\")\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "    except Exception as e:\n",
    "         display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1699326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class AgentStateSchema(TypedDict):\n",
    "    input: dict\n",
    "    sensor_data: dict\n",
    "    response: str\n",
    "\n",
    "graph = StateGraph(AgentStateSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f817ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(state: AgentStateSchema):\n",
    "    \"\"\"\n",
    "    Ingests initial input and sensor data into the state.\n",
    "    \"\"\"\n",
    "    print(\"--- ‚öôÔ∏è Ingesting Data ---\")\n",
    "    new_state = {\n",
    "        \"sensor_data\": state.get(\"input\", {}), # Assuming 'input' key from initial invoke is sensor_data\n",
    "        \"input\": {} # Clear input after processing\n",
    "    }\n",
    "    print(f\"Ingested Sensor Data: {new_state['sensor_data']}\")\n",
    "    return new_state\n",
    "\n",
    "def analyze(state: AgentStateSchema):\n",
    "    \"\"\"\n",
    "    Analyzes sensor data using the LLM and expects a JSON response.\n",
    "    \"\"\"\n",
    "    print(\"--- ‚öôÔ∏è Analyzing Data with LLM ---\")\n",
    "    sensor_data = state.get(\"sensor_data\", {})\n",
    "\n",
    "    # Construct the messages for Gemini, including the system prompt and user input\n",
    "    messages = [\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Analyze the following sensor data and provide your assessment in the specified JSON format.\n",
    "        \n",
    "        Sensor Data: {json.dumps(sensor_data, indent=2)}\n",
    "        \"\"\")\n",
    "    ]\n",
    "\n",
    "    # Invoke the LLM\n",
    "    full_content = \"\"\n",
    "    try:\n",
    "        llm_response = llm.invoke(messages)\n",
    "        full_content = llm_response.content     \n",
    "        # full_content = json.loads(llm_response.content)\n",
    "        print(f\"LLM Raw Output:\\n{full_content}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error: LLM did not return valid JSON. Error: {e}\")\n",
    "\n",
    "    # Store the full content as the response. It's expected to be a JSON string.\n",
    "    new_state = {\n",
    "        \"response\": full_content\n",
    "    }\n",
    "\n",
    "    return new_state\n",
    "\n",
    "def output(state: AgentStateSchema):\n",
    "    \"\"\"\n",
    "    Handles the final output, displaying the structured agent response.\n",
    "    \"\"\"\n",
    "    print(\"--- ‚öôÔ∏è Final Output Node ---\")\n",
    "    resp = state.get('response', '{}')\n",
    "    display_agent_response(resp)\n",
    "    return {} # No further state changes for the final node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48ad4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7cbde4320f70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Build Graph ---\n",
    "graph.add_node(\"ingest\", ingest)\n",
    "graph.add_node(\"analyze\", analyze)\n",
    "graph.add_node(\"output\", output)\n",
    "\n",
    "graph.set_entry_point(\"ingest\")\n",
    "graph.add_edge(\"ingest\", \"analyze\")\n",
    "graph.add_edge(\"analyze\", \"output\")\n",
    "graph.add_edge(\"output\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ca2dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è Ingesting Data ---\n",
      "Ingested Sensor Data: {'sensor': 'vibration_motor_3A', 'timestamp': '2025-06-18T11:47:09Z', 'value': 0.89, 'unit': 'mm/s', 'ucl': 0.9, 'lcl': 0.5, 'trend_window': [0.91, 0.94, 0.86, 0.89], 'alert_enabled': True}\n",
      "--- ‚öôÔ∏è Analyzing Data with LLM ---\n",
      "LLM Raw Output:\n",
      "@observe: New vibration reading of 0.89 mm/s for motor 3A at 2025-06-18T11:47:09Z.\n",
      "@reason: The current reading is within the UCL and LCL. However, the two previous readings in the trend window (0.94 and 0.86) show a downward trend, and the current value is close to the UCL.\n",
      "@act: Monitor the next reading closely for a potential violation of the UCL or a sustained downward trend.\n",
      "--- ‚öôÔ∏è Final Output Node ---\n",
      "===== ü§ñ AI AGENT RESPONSE =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "@observe: New vibration reading of 0.89 mm/s for motor 3A at 2025-06-18T11:47:09Z.\n",
       "@reason: The current reading is within the UCL and LCL. However, the two previous readings in the trend window (0.94 and 0.86) show a downward trend, and the current value is close to the UCL.\n",
       "@act: Monitor the next reading closely for a potential violation of the UCL or a sustained downward trend."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input': {},\n",
       " 'sensor_data': {'sensor': 'vibration_motor_3A',\n",
       "  'timestamp': '2025-06-18T11:47:09Z',\n",
       "  'value': 0.89,\n",
       "  'unit': 'mm/s',\n",
       "  'ucl': 0.9,\n",
       "  'lcl': 0.5,\n",
       "  'trend_window': [0.91, 0.94, 0.86, 0.89],\n",
       "  'alert_enabled': True},\n",
       " 'response': '@observe: New vibration reading of 0.89 mm/s for motor 3A at 2025-06-18T11:47:09Z.\\n@reason: The current reading is within the UCL and LCL. However, the two previous readings in the trend window (0.94 and 0.86) show a downward trend, and the current value is close to the UCL.\\n@act: Monitor the next reading closely for a potential violation of the UCL or a sustained downward trend.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compile and Run ---\n",
    "app = graph.compile()\n",
    "app.invoke({\"input\": sensor_data})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pipenv-3.9",
   "language": "python",
   "name": "pipenv-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
