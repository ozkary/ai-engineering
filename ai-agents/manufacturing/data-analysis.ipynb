{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976711e9",
   "metadata": {},
   "source": [
    "Manufacturing AI Agent\n",
    "\n",
    "- Install Dependencies\n",
    "  \n",
    "```bash \n",
    "pipenv shell\n",
    "pipenv install google-generativeai\n",
    "pipenv install langchain langchain-google-genai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0458a3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozkary/.local/share/virtualenvs/chat-gpt-2H0-IQSn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.messages import HumanMessage\n",
    "import google.generativeai as genai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abc6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-preview-04-17-thinking\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/veo-2.0-generate-001\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_KEY\")\n",
    "# print(GEMINI_KEY)\n",
    "\n",
    "# show available models\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini LLM\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.3,  # .1-.3 more deterministic\n",
    "    google_api_key=GEMINI_KEY\n",
    ")\n",
    "\n",
    "# Initialize short-term memory (buffer of last 5 messages)\n",
    "memory = ConversationBufferWindowMemory(k=5, return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a48ad4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for structured agent behavior\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a specialized control chart monitoring agent in a manufacturing environment.\n",
    "Your role is to observe new sensor readings and determine process stability using SPC (Statistical Process Control) logic.\n",
    "\n",
    "Use this structured output:\n",
    "@observe: Acknowledge the new reading.\n",
    "@reason: Explain trend status or SPC rule violations, referencing the trend window.\n",
    "@act: Recommend what action to take (e.g., monitor, alert supervisor, prep shutdown).\n",
    "\"\"\"\n",
    "\n",
    "# Sample input (you can swap this out for other sensor messages)\n",
    "sensor_data = {\n",
    "    \"sensor\": \"vibration_motor_3A\",\n",
    "    \"timestamp\": \"2025-06-18T11:47:09Z\",\n",
    "    \"value\": 0.89,\n",
    "    \"unit\": \"mm/s\",\n",
    "    \"ucl\": 0.90,\n",
    "    \"lcl\": 0.50,\n",
    "    \"trend_window\": [0.91, 0.94, 0.86, 0.89],\n",
    "    \"alert_enabled\": True\n",
    "}\n",
    "\n",
    "# Prepare prompt\n",
    "formatted_data = json.dumps(sensor_data, indent=2)\n",
    "prompt = f\"{SYSTEM_PROMPT}\\n\\nNew telemetry data:\\n{formatted_data}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54933c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def display_agent_response(response): \n",
    "    print(\"===== ü§ñ AI AGENT RESPONSE =====\\n\")\n",
    "    try:\n",
    "        # Convert string to dictionary\n",
    "        response_dict = json.loads(response)           \n",
    "        print(f\"\\nüì° @observe:\\n{response_dict.get('@observe', 'N/A')}\")\n",
    "        print(f\"\\nüß† @reason:\\n{response_dict.get('@reason', 'N/A')}\")\n",
    "        print(f\"\\n‚öôÔ∏è @act:\\n{response_dict.get('@act', 'N/A')}\")\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "    except Exception as e:\n",
    "         display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251aeb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ü§ñ AI AGENT RESPONSE =====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"@observe\": \"Observed new vibration reading for motor 3A: 0.89 mm/s.\",\n",
       "  \"@reason\": \"The current reading (0.89 mm/s) is within the control limits (UCL=0.9 mm/s, LCL=0.5 mm/s). However, the last four readings in the trend window are [0.91, 0.94, 0.86, 0.89]. The two most recent readings are showing an upward trend. While not a violation of standard SPC rules yet, the two previous readings were above the UCL. This warrants close monitoring.\",\n",
       "  \"@act\": \"Monitor the vibration levels closely for the next few readings. If the upward trend continues or the UCL is breached again, alert the supervisor.\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run LLM with memory context\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "# Save exchange to memory\n",
    "memory.save_context({\"input\": formatted_data}, {\"output\": response.content})\n",
    "\n",
    "# Print the response\n",
    "display_agent_response(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pipenv-3.9",
   "language": "python",
   "name": "pipenv-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
